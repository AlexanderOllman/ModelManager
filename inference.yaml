apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama3-8b-instruct
  annotations:
    autoscaling.knative.dev/target: "1"
    autoscaling.knative.dev/class: "kpa.autoscaling.knative.dev"
    autoscaling.knative.dev/minScale: "1"
    autoscaling.knative.dev/maxScale: "1"
    serving.knative.dev/scale-to-zero-grace-period: "infinite"
    serving.kserve.io/enable-auth: "false"
    serving.knative.dev/scaleToZeroPodRetention: "false"
spec:
  predictor:
    containers:
    - name: kserve-container
      image: marketplace.us1.greenlake-hpe.com/ezua/nvcr.io/nim/meta/llama3-8b-instruct:1.0.3
      env:
        - name: NIM_CACHE_PATH
          value: /mnt/models-pvc
      ports:
        - containerPort: 8000
          protocol: TCP
      resources:
        limits:
          cpu: "4"
          memory: "16Gi"
          nvidia.com/gpu: "1"
        requests:
          cpu: "2"
          memory: "16Gi"
          nvidia.com/gpu: "1"
      volumeMounts:
        - name: "model-pvc"
          mountPath: "/mnt/models-pvc"
        - name: dshm
          mountPath: /dev/shm
    volumes:
      - name: "model-pvc"
        persistentVolumeClaim:
          claimName: "models-pvc"
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: "16Gi"